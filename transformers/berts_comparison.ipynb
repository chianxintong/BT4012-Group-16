{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FoXfG82jfWq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_seq_items', None)\n",
        "\n",
        "FILE_PATH = \"/content/drive/MyDrive/BT4012 Group 16!!/\" # change the file path accordingly\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score,\n",
        "    confusion_matrix, roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# MAIN FUNCTION TO EVALUATE MULTIPLE MODEL CSV FILES\n",
        "# ============================================================\n",
        "def evaluate_model_predictions(csv_files, y_true, model_names=None, threshold=0.5):\n",
        "    \"\"\"\n",
        "    csv_files: list of csv paths\n",
        "    y_true: ground truth labels\n",
        "    model_names: optional list of readable names\n",
        "    \"\"\"\n",
        "\n",
        "    if model_names is None:\n",
        "        model_names = [f\"Model {i+1}\" for i in range(len(csv_files))]\n",
        "\n",
        "    results = {}\n",
        "    roc_data = {}\n",
        "    pr_data = {}\n",
        "    cm_data = {}\n",
        "\n",
        "    for csv, name in zip(csv_files, model_names):\n",
        "        print(f\"\\n============ Evaluating {name} ============\\n\")\n",
        "\n",
        "        df = pd.read_csv(csv)\n",
        "        y_prob = df[\"fraudulent\"].values\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred)\n",
        "        recall = recall_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        roc_auc = roc_auc_score(y_true, y_prob)\n",
        "        pr_auc = average_precision_score(y_true, y_prob)\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        results[name] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "            \"roc_auc\": roc_auc,\n",
        "            \"pr_auc\": pr_auc,\n",
        "            \"confusion_matrix\": cm\n",
        "        }\n",
        "\n",
        "        # store curve data\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "        prec_curve, rec_curve, _ = precision_recall_curve(y_true, y_prob)\n",
        "\n",
        "        roc_data[name] = (fpr, tpr)\n",
        "        pr_data[name] = (prec_curve, rec_curve)\n",
        "        cm_data[name] = cm\n",
        "\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-score: {f1:.4f}\")\n",
        "        print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "        print(f\"PR-AUC: {pr_auc:.4f}\")\n",
        "        print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "    # After computing all, generate visualisations\n",
        "    plot_roc_curves(roc_data)\n",
        "    plot_pr_curves(pr_data)\n",
        "    plot_confusion_matrices(cm_data)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# VISUALISATIONS\n",
        "# ============================================================\n",
        "\n",
        "def plot_roc_curves(roc_data):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for name, (fpr, tpr) in roc_data.items():\n",
        "        plt.plot(fpr, tpr, label=name)\n",
        "\n",
        "    plt.plot([0,1], [0,1], 'k--', alpha=0.6)\n",
        "    plt.title(\"ROC Curves — Model Comparison\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_pr_curves(pr_data):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for name, (prec, rec) in pr_data.items():\n",
        "        plt.plot(rec, prec, label=name)\n",
        "\n",
        "    plt.title(\"Precision–Recall Curves — Model Comparison\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrices(cm_data):\n",
        "    n = len(cm_data)\n",
        "    cols = 2\n",
        "    rows = (n + 1) // 2\n",
        "\n",
        "    plt.figure(figsize=(12, 5 * rows))\n",
        "\n",
        "    for i, (name, cm) in enumerate(cm_data.items(), 1):\n",
        "        plt.subplot(rows, cols, i)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f\"Confusion Matrix — {name}\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FEATURE IMPORTANCE (RF, XGBoost)\n",
        "# ============================================================\n",
        "def plot_feature_importance(model, feature_names, top_k=20, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Works for models with:\n",
        "    - model.feature_importances_  (RF, XGB, LightGBM)\n",
        "    \"\"\"\n",
        "    if not hasattr(model, \"feature_importances_\"):\n",
        "        print(f\"Model {model_name} does not support feature importance.\")\n",
        "        return\n",
        "\n",
        "    importances = model.feature_importances_\n",
        "    idx = np.argsort(importances)[::-1][:top_k]\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.barplot(x=importances[idx], y=np.array(feature_names)[idx])\n",
        "    plt.title(f\"Top {top_k} Feature Importances — {model_name}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "JVjRtADHjt4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files = [\n",
        "   \"/content/drive/MyDrive/BT4012 Group 16!!/submission_bert-base-uncased_5fold.csv\",\n",
        "   \"/content/drive/MyDrive/BT4012 Group 16!!/submission_roberta-base_5fold_bs16.csv\",\n",
        "   \"/content/drive/MyDrive/BT4012 Group 16!!/submission_distilbert-base-uncased_5fold.csv\",\n",
        "]\n",
        "\n",
        "model_names = [\"base\",\"roberta\",\"distilbert\"]\n",
        "\n",
        "test_df = pd.read_csv(f\"/content/drive/MyDrive/BT4012 Group 16!!/test_df.csv\", keep_default_na=True)\n",
        "y_true = test_df[\"fraudulent\"].values\n",
        "\n",
        "results = evaluate_model_predictions(csv_files, y_true=y_true, model_names=model_names)\n"
      ],
      "metadata": {
        "id": "2uTnTftcjvQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}