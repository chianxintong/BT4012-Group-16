{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2-mXcelegHH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_seq_items', None)\n",
        "\n",
        "FILE_PATH = \"/content/drive/MyDrive/BT4012 Group 16!!/\" # change the file path accordingly\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKzUfPObAhis"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_job_df(\n",
        "    df,\n",
        "    cols_to_drop=None,\n",
        "    cols_to_merge=None,\n",
        "    education_col=\"required_education\",\n",
        "    new_text_col=\"text\"\n",
        "):\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1. Drop unwanted columns\n",
        "    if cols_to_drop:\n",
        "        df = df.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "    # 2. Handle missing values\n",
        "    #    - keep NaN for all\n",
        "    #    - convert \"unspecified\" (any casing) in required_education to NaN\n",
        "    if education_col in df.columns:\n",
        "        df[education_col] = df[education_col].apply(\n",
        "            lambda x: np.nan if isinstance(x, str) and x.strip().lower() == \"unspecified\" else x\n",
        "        )\n",
        "\n",
        "    # 3. Concatenate text columns\n",
        "    if cols_to_merge:\n",
        "        # Replace NaN with empty string\n",
        "        df[cols_to_merge] = df[cols_to_merge].fillna(\"\")\n",
        "\n",
        "        # Merge into new column\n",
        "        df[new_text_col] = df[cols_to_merge].astype(str).agg(\" \".join, axis=1)\n",
        "\n",
        "        # 4. Drop original columns\n",
        "        df = df.drop(columns=cols_to_merge, errors=\"ignore\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JieHVXXEAu4c"
      },
      "outputs": [],
      "source": [
        "cols_to_drop = [\n",
        "    \"job_id\",\n",
        "    \"salary_range\",\n",
        "    \"telecommuting\",\n",
        "    \"has_company_logo\",\n",
        "    \"has_questions\",\n",
        "]\n",
        "\n",
        "cols_to_merge = [\n",
        "    \"title\",\n",
        "    \"location\",\n",
        "    \"department\",\n",
        "    \"company_profile\",\n",
        "    \"description\",\n",
        "    \"requirements\",\n",
        "    \"benefits\",\n",
        "    \"employment_type\",\n",
        "    \"required_experience\",\n",
        "    \"required_education\",\n",
        "    \"industry\",\n",
        "    \"function\"\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2HzJQuHBs2X"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def clean(df, text_col=\"text\"):\n",
        "    \"\"\"\n",
        "    Cleans a dataframe by:\n",
        "    - stripping whitespace for all object (string) columns\n",
        "    - converting empty strings to NaN\n",
        "    - removing URLs from the specified text column\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "        text_col (str): Column to apply URL cleaning to\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Cleaned dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Strip leading/trailing spaces from all object columns\n",
        "    df = df.apply(lambda col: col.str.strip() if col.dtype == \"object\" else col)\n",
        "\n",
        "    # 2. Convert empty strings to NaN\n",
        "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "    # 3. Remove URLs from the main text column\n",
        "    if text_col in df.columns:\n",
        "        df[text_col] = df[text_col].astype(str).apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnus8WJPLrgm"
      },
      "source": [
        "# Base Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJZ4irJBLksX"
      },
      "outputs": [],
      "source": [
        "# BASE BERT 5-FOLDS AND FINE TUNING\n",
        "\n",
        "import re, time, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "\n",
        "# BASE BERT CLASSIFIER MODEL\n",
        "class TextDetector(nn.Module):\n",
        "    def __init__(self, model_name=\"bert-base-uncased\", dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # BERT uses pooled CLS output at outputs.pooler_output\n",
        "        pooled = outputs.pooler_output\n",
        "        pooled = self.dropout(pooled)\n",
        "\n",
        "        logits = self.classifier(pooled)\n",
        "        return logits\n",
        "\n",
        "    def predict(self, tokenizer, texts, device='cuda'):\n",
        "        self.eval()\n",
        "        device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(device)\n",
        "\n",
        "        all_probs = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(texts), 4):\n",
        "                batch = texts[i:i+4]\n",
        "                inputs = tokenizer(\n",
        "                    batch, return_tensors='pt',\n",
        "                    truncation=True, padding=True, max_length=512\n",
        "                ).to(device)\n",
        "\n",
        "                if 'token_type_ids' in inputs:\n",
        "                    del inputs['token_type_ids']\n",
        "\n",
        "                logits = self(**inputs)\n",
        "                probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "                all_probs.extend(probs)\n",
        "\n",
        "                del inputs, logits, probs\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return np.array(all_probs)\n",
        "\n",
        "# TRAINING FUNCTION\n",
        "def train_detector(model, tokenizer, train_texts, train_labels,\n",
        "                   val_texts, val_labels,\n",
        "                   lr=1e-5, epochs=5, batch_size=16, patience=2):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    def tokenize_batch(texts, labels):\n",
        "        enc = tokenizer(texts, truncation=True, padding=True,\n",
        "                        max_length=512, return_tensors='pt')\n",
        "        if 'token_type_ids' in enc:\n",
        "            del enc['token_type_ids']\n",
        "        return enc['input_ids'], enc['attention_mask'], torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    train_ids, train_mask, train_y = tokenize_batch(train_texts, train_labels)\n",
        "    val_ids, val_mask, val_y = tokenize_batch(val_texts, val_labels)\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(train_ids, train_mask, train_y),\n",
        "                              batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(val_ids, val_mask, val_y),\n",
        "                            batch_size=batch_size)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # class imbalance handling\n",
        "    pos_weight_value = (train_labels.count(0) / train_labels.count(1))\n",
        "    pos_weight = torch.tensor([pos_weight_value], device=device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # scheduler\n",
        "    num_training_steps = len(train_loader) * epochs\n",
        "    warmup_steps = int(0.1 * num_training_steps)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    best_auc = 0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for input_ids, mask, labels in train_loader:\n",
        "            input_ids, mask, labels = input_ids.to(device), mask.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(input_ids, mask).squeeze(-1)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        preds, probs, gold = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for input_ids, mask, labels in val_loader:\n",
        "                input_ids, mask = input_ids.to(device), mask.to(device)\n",
        "                logits = model(input_ids, mask).squeeze(-1)\n",
        "                p = torch.sigmoid(logits)\n",
        "\n",
        "                preds.extend((p > 0.5).int().cpu().numpy())\n",
        "                probs.extend(p.cpu().numpy())\n",
        "                gold.extend(labels.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(gold, preds)\n",
        "        prec = precision_score(gold, preds)\n",
        "        rec = recall_score(gold, preds)\n",
        "        f1 = f1_score(gold, preds)\n",
        "        roc_auc = roc_auc_score(gold, probs)\n",
        "        pr_auc = average_precision_score(gold, probs)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}: \"\n",
        "            f\"Acc={acc:.4f} Prec={prec:.4f} Rec={rec:.4f} \"\n",
        "            f\"F1={f1:.4f} ROC_AUC={roc_auc:.4f} PR_AUC={pr_auc:.4f}\"\n",
        "        )\n",
        "\n",
        "        if pr_auc > best_auc:\n",
        "            best_auc = pr_auc\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epoch > 0 and epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping. Best PR_AUC={best_auc:.4f}\")\n",
        "                model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "                break\n",
        "\n",
        "    print(f\"Training complete. Best PR_AUC={best_auc:.4f}\")\n",
        "    return model\n",
        "\n",
        "# K-FOLD TRAINING\n",
        "def run_kfold_training(train_df, tokenizer, model_name=\"bert-base-uncased\",\n",
        "                       lr=1e-5, epochs=5, batch_size=16, patience=2, n_splits=5):\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    oof_preds = np.zeros(len(train_df))\n",
        "    fold_aucs = []\n",
        "    models = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df['text'], train_df['fraudulent'])):\n",
        "        print(f\"\\nFold {fold+1}/{n_splits}\")\n",
        "\n",
        "        tr_df = train_df.iloc[train_idx]\n",
        "        val_df = train_df.iloc[val_idx]\n",
        "\n",
        "        model = TextDetector(model_name=model_name)\n",
        "\n",
        "        trained_model = train_detector(\n",
        "            model, tokenizer,\n",
        "            tr_df['text'].tolist(), tr_df['fraudulent'].tolist(),\n",
        "            val_df['text'].tolist(), val_df['fraudulent'].tolist(),\n",
        "            lr=lr, epochs=epochs, batch_size=batch_size, patience=patience\n",
        "        )\n",
        "\n",
        "        val_probs = trained_model.predict(tokenizer, val_df['text'].tolist(), device=device)\n",
        "        oof_preds[val_idx] = val_probs\n",
        "\n",
        "        fold_auc = average_precision_score(val_df[\"fraudulent\"], val_probs)\n",
        "        fold_aucs.append(fold_auc)\n",
        "\n",
        "        print(f\"Fold {fold+1} PR_AUC={fold_auc:.4f}\")\n",
        "\n",
        "        torch.save(trained_model.state_dict(), f\"bert_fold{fold+1}.pt\")\n",
        "        models.append(trained_model)\n",
        "\n",
        "        del tr_df, val_df, trained_model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\nCV Results\")\n",
        "    print(\"Fold AUCs:\", [round(x, 4) for x in fold_aucs])\n",
        "    print(f\"Mean AUC = {np.mean(fold_aucs):.4f}\")\n",
        "\n",
        "    return models, np.mean(fold_aucs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main\n"
      ],
      "metadata": {
        "id": "bD6x0L72SJDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iBkNUvV71xp"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    test_df = pd.read_csv(f\"/content/drive/MyDrive/BT4012 Group 16!!/test_df.csv\", keep_default_na=True) # automatically treat certain null values as \"NaN\"\n",
        "    train_df = pd.read_csv(f\"/content/drive/MyDrive/BT4012 Group 16!!/train_df.csv\", keep_default_na=True)\n",
        "    train_df = preprocess_job_df(train_df, cols_to_drop, cols_to_merge)\n",
        "    test_df = preprocess_job_df(test_df, cols_to_drop, cols_to_merge)\n",
        "    train_df = clean(train_df, text_col=\"text\")\n",
        "    test_df = clean(test_df, text_col=\"text\")\n",
        "\n",
        "    if 'fraudulent' in test_df.columns:\n",
        "      test_df = test_df.drop(columns=['fraudulent'])\n",
        "\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model_shortname = model_name.split(\"/\")[-1]\n",
        "\n",
        "    start = time.time()\n",
        "    models, mean_auc = run_kfold_training(\n",
        "        train_df, tokenizer, model_name=model_name,\n",
        "        lr=1e-5, epochs=5, batch_size=16, patience=2, n_splits=5\n",
        "    )\n",
        "    print(f\"\\nTraining done in {(time.time()-start)/60:.1f} min\")\n",
        "\n",
        "    all_test_probs = []\n",
        "    for i, model in enumerate(models):\n",
        "        print(f\"Predicting with fold {i+1} model...\")\n",
        "        probs = model.predict(tokenizer, test_df['text'].tolist())\n",
        "        all_test_probs.append(probs)\n",
        "\n",
        "    final_probs = np.mean(all_test_probs, axis=0)\n",
        "\n",
        "    submission = pd.DataFrame({'id': range(len(final_probs)), 'fraudulent': final_probs})\n",
        "    path = f\"/content/drive/MyDrive/BT4012 Group 16!!/submission_{model_shortname}_5fold.csv\"\n",
        "    submission.to_csv(path, index=False)\n",
        "    print(f\"\\n Submission saved to {path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}