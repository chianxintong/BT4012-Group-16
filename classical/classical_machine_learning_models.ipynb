{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Set-Up"
      ],
      "metadata": {
        "id": "BsUXkQS_ZQ1y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1N0BYigVa_I"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve, auc\n",
        "import warnings\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score, # AUC-PR\n",
        "    classification_report\n",
        ")\n",
        "import time\n",
        "from joblib import parallel_backend\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = \"/content/drive/MyDrive/BT4012 Group 16!!/\" # change the file path accordingly\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QPoL2t0mVuYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the stratified 80/20 train and test datasets that have already undergone data cleaning, feature engineering and feature selection. Right-skewed numerical features have been log-transformed and categorical features have been one-hot encoded."
      ],
      "metadata": {
        "id": "euEKUnrmBonS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(f\"{FILE_PATH}ohe_selected_train_df.csv\", keep_default_na=True)\n",
        "\n",
        "# Check\n",
        "print(\"Dataset Size:\")\n",
        "print(train_df.shape)\n",
        "\n",
        "train_df.head(5)"
      ],
      "metadata": {
        "id": "tvGDbziFVviK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(f\"{FILE_PATH}ohe_selected_test_df.csv\", keep_default_na=True)\n",
        "\n",
        "# Check\n",
        "print(\"Dataset Size:\")\n",
        "print(test_df.shape)\n",
        "\n",
        "test_df.head(5)"
      ],
      "metadata": {
        "id": "-lQAwPuKsH7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preparing Numerical and Categorical Columns for Model Training"
      ],
      "metadata": {
        "id": "SUqarCJEuthb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final categorical columns (after OHE)\n",
        "catergorical_cols = ['company_profile_missing',\n",
        " 'has_questions',\n",
        " 'description_contains_remote_keywords',\n",
        " 'department_missing',\n",
        " 'title_contains_action_word',\n",
        " 'title_seniority_level',\n",
        " 'US_listing_without_state_info',\n",
        " 'benefits_contains_remote_keywords',\n",
        " 'title_contains_remote_keywords',\n",
        " 'telecommuting',\n",
        " 'company_profile_contains_remote_keywords',\n",
        " 'department_invalid_dept',\n",
        " 'industry_Accounting',\n",
        " 'industry_Airlines/Aviation',\n",
        " 'industry_Apparel & Fashion',\n",
        " 'industry_Automotive',\n",
        " 'industry_Banking',\n",
        " 'industry_Building Materials',\n",
        " 'industry_Civic & Social Organization',\n",
        " 'industry_Computer Games',\n",
        " 'industry_Computer Software',\n",
        " 'industry_Construction',\n",
        " 'industry_Consumer Electronics',\n",
        " 'industry_Consumer Goods',\n",
        " 'industry_Consumer Services',\n",
        " 'industry_Cosmetics',\n",
        " 'industry_Design',\n",
        " 'industry_E-Learning',\n",
        " 'industry_Education Management',\n",
        " 'industry_Electrical/Electronic Manufacturing',\n",
        " 'industry_Entertainment',\n",
        " 'industry_Environmental Services',\n",
        " 'industry_Events Services',\n",
        " 'industry_Facilities Services',\n",
        " 'industry_Financial Services',\n",
        " 'industry_Food & Beverages',\n",
        " 'industry_Health, Wellness and Fitness',\n",
        " 'industry_Hospital & Health Care',\n",
        " 'industry_Hospitality',\n",
        " 'industry_Human Resources',\n",
        " 'industry_Information Technology and Services',\n",
        " 'industry_Insurance',\n",
        " 'industry_Internet',\n",
        " 'industry_Legal Services',\n",
        " 'industry_Leisure, Travel & Tourism',\n",
        " 'industry_Logistics and Supply Chain',\n",
        " 'industry_Management Consulting',\n",
        " 'industry_Marketing and Advertising',\n",
        " 'industry_Media Production',\n",
        " 'industry_Medical Practice',\n",
        " 'industry_Nonprofit Organization Management',\n",
        " 'industry_Oil & Energy',\n",
        " 'industry_Online Media',\n",
        " 'industry_Public Relations and Communications',\n",
        " 'industry_RARE CATEGORY',\n",
        " 'industry_Real Estate',\n",
        " 'industry_Restaurants',\n",
        " 'industry_Retail',\n",
        " 'industry_Staffing and Recruiting',\n",
        " 'industry_Telecommunications',\n",
        " 'industry_UNKNOWN',\n",
        " 'industry_Warehousing',\n",
        " 'country_AU',\n",
        " 'country_CA',\n",
        " 'country_DE',\n",
        " 'country_GB',\n",
        " 'country_GR',\n",
        " 'country_IN',\n",
        " 'country_NL',\n",
        " 'country_NZ',\n",
        " 'country_PH',\n",
        " 'country_RARE CATEGORY',\n",
        " 'country_UNKNOWN',\n",
        " 'country_US',\n",
        " 'function_Accounting/Auditing',\n",
        " 'function_Administrative',\n",
        " 'function_Art/Creative',\n",
        " 'function_Business Development',\n",
        " 'function_Consulting',\n",
        " 'function_Customer Service',\n",
        " 'function_Design',\n",
        " 'function_Education',\n",
        " 'function_Engineering',\n",
        " 'function_Finance',\n",
        " 'function_Health Care Provider',\n",
        " 'function_Human Resources',\n",
        " 'function_Information Technology',\n",
        " 'function_Management',\n",
        " 'function_Marketing',\n",
        " 'function_Other',\n",
        " 'function_Project Management',\n",
        " 'function_RARE CATEGORY',\n",
        " 'function_Sales',\n",
        " 'function_UNKNOWN',\n",
        " 'function_Writing/Editing',\n",
        " 'required_education_Associate Degree',\n",
        " \"required_education_Bachelor's Degree\",\n",
        " 'required_education_Certification',\n",
        " 'required_education_Doctorate',\n",
        " 'required_education_High School or equivalent',\n",
        " \"required_education_Master's Degree\",\n",
        " 'required_education_Professional',\n",
        " 'required_education_Some College Coursework Completed',\n",
        " 'required_education_Some High School Coursework',\n",
        " 'required_education_UNKNOWN',\n",
        " 'required_education_Unspecified',\n",
        " 'required_education_Vocational',\n",
        " 'required_education_Vocational - Degree',\n",
        " 'required_education_Vocational - HS Diploma',\n",
        " 'required_experience_Associate',\n",
        " 'required_experience_Director',\n",
        " 'required_experience_Entry level',\n",
        " 'required_experience_Executive',\n",
        " 'required_experience_Internship',\n",
        " 'required_experience_Mid-Senior level',\n",
        " 'required_experience_Not Applicable',\n",
        " 'required_experience_UNKNOWN',\n",
        " 'employment_type_Contract',\n",
        " 'employment_type_Full-time',\n",
        " 'employment_type_Other',\n",
        " 'employment_type_Part-time',\n",
        " 'employment_type_Temporary',\n",
        " 'employment_type_UNKNOWN']\n",
        "\n",
        "# Check\n",
        "X_train_categorical = train_df[catergorical_cols].values\n",
        "X_test_categorical  = test_df[catergorical_cols].values\n",
        "print(\"Categorical (OHE) shape:\", X_train_categorical.shape)\n",
        "\n",
        "\n",
        "# Final numerical columns (after log-transformed)\n",
        "numerical_cols = ['requirements_special_char_ratio',\n",
        " 'department_avg_word_length_log',\n",
        " 'company_profile_POS_CONJ_normalised',\n",
        " 'requirements_all_caps_words_log',\n",
        " 'description_POS_ADJ_normalised',\n",
        " 'requirements_unique_char_normalised',\n",
        " 'company_profile_POS_ADJ_normalised',\n",
        " 'requirements_longest_repeated_chars_length_log',\n",
        " 'requirements_char_count_log',\n",
        " 'description_POS_PRON_normalised',\n",
        " 'description_flesch_score',\n",
        " 'requirements_exclamation_normalised',\n",
        " 'requirements_POS_NUM_normalised',\n",
        " 'requirements_POS_PRON_normalised',\n",
        " 'company_profile_POS_NOUN_normalised',\n",
        " 'location_segment_count_log',\n",
        " 'benefits_POS_PRON_normalised',\n",
        " 'description_sentence_count_log',\n",
        " 'description_POS_NOUN_normalised',\n",
        " 'benefits_has_email',\n",
        " 'company_profile_POS_DET_normalised',\n",
        " 'description_POS_DET_normalised',\n",
        " 'company_profile_avg_word_length_log',\n",
        " 'requirements_POS_CONJ_normalised',\n",
        " 'department_special_char_ratio',\n",
        " 'description_exclamation_normalised',\n",
        " 'company_profile_duplicated',\n",
        " 'benefits_unique_char_normalised',\n",
        " 'requirements_gunning_fog_log',\n",
        " 'requirements_sentiment_compound',\n",
        " 'requirements_avg_word_length',\n",
        " 'description_unique_char_normalised',\n",
        " 'benefits_POS_PRT_normalised',\n",
        " 'description_uppercase_ratio',\n",
        " 'requirements_has_url',\n",
        " 'company_profile_POS_PRT_normalised',\n",
        " 'description_has_email',\n",
        " 'requirements_unique_word_normalised',\n",
        " 'benefits_avg_sentence_length_log',\n",
        " 'company_profile_sentiment_compound',\n",
        " 'description_POS_VERB_normalised',\n",
        " 'title_avg_word_length_log',\n",
        " 'description_POS_ADP_normalised',\n",
        " 'benefits_spelling_error_rate',\n",
        " 'requirements_duplicated',\n",
        " 'requirements_has_email',\n",
        " 'benefits_POS_X_normalised',\n",
        " 'benefits_POS_ADJ_normalised',\n",
        " 'company_profile_unique_char_normalised',\n",
        " 'title_unique_word_normalised',\n",
        " 'title_unique_char_normalised',\n",
        " 'description_sentiment_neg',\n",
        " 'benefits_gunning_fog_log',\n",
        " 'benefits_question_normalised',\n",
        " 'requirements_POS_NOUN_normalised',\n",
        " 'description_special_char_ratio',\n",
        " 'description_sentiment_pos',\n",
        " 'description_question_normalised',\n",
        " 'benefits_POS_NOUN_normalised',\n",
        " 'company_profile_question_count_log',\n",
        " 'company_profile_avg_sentence_length',\n",
        " 'description_avg_sentence_length_log',\n",
        " 'benefits_POS_CONJ_normalised',\n",
        " 'company_profile_POS_PRON_normalised',\n",
        " 'description_sentiment_compound',\n",
        " 'benefits_POS_ADP_normalised',\n",
        " 'benefits_POS_DET_normalised',\n",
        " 'company_profile_char_count_log',\n",
        " 'requirements_spelling_error_rate',\n",
        " 'title_avg_sentence_length_log',\n",
        " 'company_profile_flesch_score',\n",
        " 'company_profile_sentiment_pos',\n",
        " 'benefits_exclamation_normalised',\n",
        " 'description_objective_score',\n",
        " 'description_digit_ratio',\n",
        " 'description_POS_ADV_normalised',\n",
        " 'requirements_avg_sentence_length_log',\n",
        " 'company_profile_has_phone',\n",
        " 'benefits_longest_repeated_chars_length_log',\n",
        " 'benefits_has_url',\n",
        " 'company_profile_unique_word_normalised',\n",
        " 'company_profile_sentence_count_log',\n",
        " 'requirements_POS_ADJ_normalised',\n",
        " 'requirements_POS_VERB_normalised',\n",
        " 'title_char_count_log',\n",
        " 'benefits_sentiment_compound',\n",
        " 'company_profile_exclamation_normalised',\n",
        " 'benefits_sentiment_pos',\n",
        " 'benefits_char_count_log',\n",
        " 'benefits_avg_word_length_log',\n",
        " 'description_all_caps_words_normalised',\n",
        " 'requirements_flesch_score',\n",
        " 'department_unique_char_normalised',\n",
        " 'company_profile_gunning_fog',\n",
        " 'title_digit_ratio',\n",
        " 'benefits_unique_word_normalised',\n",
        " 'company_profile_POS_NUM_normalised',\n",
        " 'company_profile_uppercase_ratio',\n",
        " 'benefits_special_char_ratio',\n",
        " 'benefits_digit_ratio',\n",
        " 'requirements_sentiment_pos',\n",
        " 'company_profile_special_char_ratio',\n",
        " 'title_special_char_ratio',\n",
        " 'description_duplicated',\n",
        " 'company_profile_objective_score',\n",
        " 'company_profile_digit_ratio',\n",
        " 'description_spelling_error_rate',\n",
        " 'benefits_objective_score',\n",
        " 'company_profile_POS_ADP_normalised',\n",
        " 'benefits_POS_NUM_normalised',\n",
        " 'description_all_caps_words_log',\n",
        " 'benefits_uppercase_ratio',\n",
        " 'requirements_uppercase_ratio',\n",
        " 'requirements_sentiment_neg',\n",
        " 'requirements_POS_X_normalised',\n",
        " 'benefits_all_caps_words_normalised',\n",
        " 'description_unique_word_normalised',\n",
        " 'requirements_digit_ratio',\n",
        " 'benefits_sentiment_neg',\n",
        " 'benefits_flesch_score',\n",
        " 'description_POS_NUM_normalised',\n",
        " 'description_gunning_fog_log',\n",
        " 'company_profile_spelling_error_rate',\n",
        " 'description_avg_word_length_log',\n",
        " 'requirements_question_count_log',\n",
        " 'benefits_POS_VERB_normalised',\n",
        " 'requirements_POS_ADP_normalised',\n",
        " 'company_profile_sentiment_neg',\n",
        " 'requirements_objective_score',\n",
        " 'requirements_POS_PRT_normalised',\n",
        " 'benefits_POS_ADV_normalised',\n",
        " 'benefits_sentence_count_log',\n",
        " 'description_POS_CONJ_normalised',\n",
        " 'description_POS_PRT_normalised',\n",
        " 'requirements_sentence_count_log',\n",
        " 'description_has_phone',\n",
        " 'description_POS_X_normalised',\n",
        " 'company_profile_POS_X_normalised',\n",
        " 'company_profile_all_caps_words_normalised',\n",
        " 'company_profile_longest_repeated_chars_length_log',\n",
        " 'company_profile_POS_ADV_normalised',\n",
        " 'title_longest_repeated_chars_length',\n",
        " 'requirements_POS_ADV_normalised',\n",
        " 'requirements_POS_DET_normalised',\n",
        " 'requirements_all_caps_words_normalised',\n",
        " 'company_profile_POS_VERB_normalised',\n",
        " 'description_longest_repeated_chars_length_log']\n",
        "\n",
        "# Scale only the numerical columns\n",
        "num_scaler = StandardScaler()\n",
        "\n",
        "# Fit on train, transform train & test\n",
        "X_train_numerical = num_scaler.fit_transform(train_df[numerical_cols])\n",
        "X_test_numerical  = num_scaler.transform(test_df[numerical_cols])\n",
        "\n",
        "# Check\n",
        "print(\"Scaled numeric shape:\", X_train_numerical.shape)\n",
        "\n",
        "# In total, there are 276 columns which is made up of 123 categorical columns, 147 numerical columns,\n",
        "# 4 text columns, 1 id column ('job_id') and 1 target column ('fraudulent')."
      ],
      "metadata": {
        "id": "99YfpfutuoXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Creating Text Embedding with Bert"
      ],
      "metadata": {
        "id": "dZZAH3qpnYkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_COLS = [\"company_profile_noencoded\", \"description_noencoded\", \"requirements_noencoded\", \"benefits_noencoded\"]\n",
        "\n",
        "def concat_text_columns(df, text_cols=TEXT_COLS):\n",
        "    # Replace NaNs with empty strings, then join with a separator\n",
        "    return (\n",
        "        df[text_cols]\n",
        "        .fillna(\"\")\n",
        "        .agg(\" [SEP] \".join, axis=1)\n",
        "    )\n",
        "\n",
        "# Build full_text for train & test\n",
        "train_df[\"full_text\"] = concat_text_columns(train_df)\n",
        "test_df[\"full_text\"]  = concat_text_columns(test_df)"
      ],
      "metadata": {
        "id": "2yVhzZLG1w5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "dhZfSzDLMGqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bert_embeddings(texts, tokenizer, model, device, batch_size, max_length):\n",
        "    all_embeddings = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # tqdm to show progress bar\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding Batches\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=max_length,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded)\n",
        "            token_embeddings = outputs.last_hidden_state\n",
        "\n",
        "            attention_mask = (\n",
        "                encoded[\"attention_mask\"]\n",
        "                .unsqueeze(-1)\n",
        "                .expand(token_embeddings.size())\n",
        "                .float()\n",
        "            )\n",
        "\n",
        "            sum_embeddings = torch.sum(token_embeddings * attention_mask, dim=1)\n",
        "            sum_mask = torch.clamp(attention_mask.sum(dim=1), min=1e-9)\n",
        "            mean_pooled = sum_embeddings / sum_mask\n",
        "\n",
        "        all_embeddings.append(mean_pooled.cpu().numpy())\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTotal embedding time: {total_time:.2f} seconds \"\n",
        "          f\"({total_time/60:.2f} minutes)\")\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# Actually compute the text embeddings\n",
        "texts_train = train_df[\"full_text\"].fillna(\"\").tolist()\n",
        "texts_test  = test_df[\"full_text\"].fillna(\"\").tolist()\n",
        "\n",
        "train_embeddings = compute_bert_embeddings(\n",
        "    texts_train, tokenizer, model, device=device, batch_size=16, max_length=512\n",
        ")\n",
        "\n",
        "test_embeddings = compute_bert_embeddings(\n",
        "    texts_test, tokenizer, model, device=device, batch_size=16, max_length=512\n",
        ")\n",
        "\n",
        "print(\"Train embeddings shape:\", train_embeddings.shape)\n",
        "print(\"Test embeddings shape:\",  test_embeddings.shape)"
      ],
      "metadata": {
        "id": "HBHomaNj9WOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA to reduce dimensionality"
      ],
      "metadata": {
        "id": "Bhs9f0kQBhXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep 90% variance\n",
        "pca = PCA(n_components=0.90, random_state=42)\n",
        "\n",
        "# Fit PCA on TRAIN only\n",
        "pca.fit(train_embeddings)\n",
        "\n",
        "# Transform train and test\n",
        "X_train_embeddings = pca.transform(train_embeddings)\n",
        "X_test_embeddings  = pca.transform(test_embeddings)\n",
        "\n",
        "print(\"Num components:\", pca.n_components_)\n",
        "print(\"PCA train shape:\", X_train_embeddings.shape)\n",
        "print(\"PCA test shape :\", X_test_embeddings.shape)\n",
        "print(\"Explained variance ratio sum:\", pca.explained_variance_ratio_.sum())"
      ],
      "metadata": {
        "id": "2sEusLiGCWGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Concatenating the final features with text embeddings"
      ],
      "metadata": {
        "id": "qvZN-aHw_Bht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_final = np.hstack([X_train_numerical, X_train_categorical, X_train_embeddings])\n",
        "X_test_final  = np.hstack([X_test_numerical,  X_test_categorical,  X_test_embeddings])\n",
        "\n",
        "y_train = train_df[\"fraudulent\"].values\n",
        "y_test  = test_df[\"fraudulent\"].values\n",
        "\n",
        "print(\"X_train_final:\", X_train_final.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "\n",
        "print(\"X_test_final:\", X_test_final.shape)\n",
        "print(\"y_test:\", y_test.shape)"
      ],
      "metadata": {
        "id": "nhbntHqp0urB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Preparing for Model Training"
      ],
      "metadata": {
        "id": "8DBHQU96klnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stratified Cross Validation Helper Function"
      ],
      "metadata": {
        "id": "PfCfGqM1vFYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cv_grid(\n",
        "    estimator,\n",
        "    X,\n",
        "    y,\n",
        "    param_grid,\n",
        "    *,\n",
        "    model_name=None,\n",
        "    n_splits=5,\n",
        "    scoring=\"average_precision\",\n",
        "    n_jobs=-1,\n",
        "    refit=True, # Fit best model at the end\n",
        "    verbose=2,\n",
        "):\n",
        "    name = model_name or estimator.__class__.__name__\n",
        "\n",
        "    cv = StratifiedKFold(\n",
        "        n_splits=n_splits,\n",
        "        shuffle=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    combos = list(ParameterGrid(param_grid))\n",
        "    print(f\"\\n {name}: {len(combos)} candidates × {n_splits} folds = {len(combos) * n_splits} fits \")\n",
        "    print(f\"Scoring = {scoring}\\n\")\n",
        "\n",
        "    grid = GridSearchCV(\n",
        "        estimator=estimator,\n",
        "        param_grid=param_grid,\n",
        "        cv=cv,\n",
        "        scoring=scoring,\n",
        "        n_jobs=n_jobs,\n",
        "        refit=refit,\n",
        "        verbose=verbose,\n",
        "        return_train_score=False,\n",
        "        error_score=\"raise\"\n",
        "    )\n",
        "\n",
        "    t0 = time.time()\n",
        "    with parallel_backend(\"threading\"):\n",
        "        grid.fit(X, y)\n",
        "    total_min = (time.time() - t0) / 60.0\n",
        "\n",
        "    print(f\"\\n[{name}] total CV time: {total_min:.2f} min\")\n",
        "    print(f\"[{name}] best {scoring}: {grid.best_score_:.5f}\")\n",
        "    print(f\"[{name}] best params: {grid.best_params_}\")\n",
        "\n",
        "    return grid"
      ],
      "metadata": {
        "id": "SLlE3vw-lu_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weights to Handle Class Imbalance"
      ],
      "metadata": {
        "id": "1BoeyPy_Ud9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Class imbalance handling (with full train labels, not fold-specific)\n",
        "# To be reused for all base models\n",
        "neg, pos = (y_train == 0).sum(), (y_train == 1).sum()\n",
        "scale_pos = float(neg) / max(float(pos), 1.0)\n",
        "print(f\"scale_pos_weight (neg/pos) = {scale_pos:.3f}\")"
      ],
      "metadata": {
        "id": "Qf0JJACmUhiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Base ML Models"
      ],
      "metadata": {
        "id": "HcGPdbmSfPou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Logistic Regression"
      ],
      "metadata": {
        "id": "6kjLdzRkl8JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_lr = LogisticRegression(\n",
        "    penalty=\"l2\",\n",
        "    max_iter=5000,\n",
        "    solver=\"saga\",\n",
        "    n_jobs=1,\n",
        "    class_weight={0: 1.0, 1: scale_pos},\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "param_grid_lr = {\n",
        "    \"C\": [0.5, 1.0, 2.0],\n",
        "}\n",
        "\n",
        "grid_lr = run_cv_grid(\n",
        "    estimator=base_lr,\n",
        "    X=X_train_final,\n",
        "    y=y_train,\n",
        "    param_grid=param_grid_lr,\n",
        "    model_name=\"LogisticRegression\",\n",
        "    scoring=\"average_precision\",\n",
        ")\n",
        "\n",
        "best_lr = grid_lr.best_estimator_"
      ],
      "metadata": {
        "id": "DjwaxOV1d5Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_proba = best_lr.predict_proba(X_test_final)[:, 1]\n",
        "y_test_pred  = (y_test_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n Logistic Regression: Test Metrics\")\n",
        "print(f\"ROC-AUC : {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
        "print(f\"AUC-PR  : {average_precision_score(y_test, y_test_proba):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Recall  : {recall_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"F1      : {f1_score(y_test, y_test_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "74hWml_1hiCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Support Vector Machine"
      ],
      "metadata": {
        "id": "9tsCia42l-n5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_svm = SVC(\n",
        "    kernel=\"linear\",\n",
        "    probability=True,\n",
        "    class_weight={0: 1.0, 1: scale_pos},\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "param_grid_svm = {\n",
        "    \"C\": [1.0, 2.0],\n",
        "}\n",
        "\n",
        "grid_svm = run_cv_grid(\n",
        "    estimator=base_svm,\n",
        "    X=X_train_final,\n",
        "    y=y_train,\n",
        "    param_grid=param_grid_svm,\n",
        "    model_name=\"LinearSVM\",\n",
        "    scoring=\"average_precision\",\n",
        ")\n",
        "\n",
        "best_svm = grid_svm.best_estimator_"
      ],
      "metadata": {
        "id": "q-JwUicKfSK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_proba = best_svm.predict_proba(X_test_final)[:, 1]\n",
        "y_test_pred  = (y_test_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n SVM: Test Metrics\")\n",
        "print(f\"ROC-AUC : {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
        "print(f\"AUC-PR  : {average_precision_score(y_test, y_test_proba):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Recall  : {recall_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"F1      : {f1_score(y_test, y_test_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "IE93RRO55BGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c. Random Forest (Bagging)"
      ],
      "metadata": {
        "id": "p-lx_tYbmATI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    n_jobs=-1,\n",
        "    class_weight={0: 1.0, 1: scale_pos},\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "param_grid_rf = {\n",
        "    \"n_estimators\": [300, 500],\n",
        "    \"max_depth\": [None, 10],\n",
        "    \"min_samples_leaf\": [1, 2],\n",
        "}\n",
        "\n",
        "grid_rf = run_cv_grid(\n",
        "    estimator=base_rf,\n",
        "    X=X_train_final,\n",
        "    y=y_train,\n",
        "    param_grid=param_grid_rf,\n",
        "    model_name=\"RandomForest\",\n",
        "    scoring=\"average_precision\",\n",
        ")\n",
        "\n",
        "best_rf = grid_rf.best_estimator_"
      ],
      "metadata": {
        "id": "ZX2-PBg7Dn0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_proba = best_rf.predict_proba(X_test_final)[:, 1]\n",
        "y_test_pred  = (y_test_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n Random Forest: Test Metrics\")\n",
        "print(f\"ROC-AUC : {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
        "print(f\"AUC-PR  : {average_precision_score(y_test, y_test_proba):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Recall  : {recall_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"F1      : {f1_score(y_test, y_test_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "of-MTvHAcf3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d. XGBoost / LightGBM (Boosting)"
      ],
      "metadata": {
        "id": "pr_UngXymKwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_lgb = LGBMClassifier(\n",
        "    device=\"gpu\",\n",
        "    scale_pos_weight=scale_pos,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "param_grid_lgb = {\n",
        "    \"n_estimators\": [200, 500],\n",
        "    \"learning_rate\": [0.05, 0.1],\n",
        "    \"num_leaves\": [31, 64],\n",
        "}\n",
        "\n",
        "grid_lgb = run_cv_grid(\n",
        "    estimator=base_lgb,\n",
        "    X=X_train_final,\n",
        "    y=y_train,\n",
        "    param_grid=param_grid_lgb,\n",
        "    model_name=\"LightGBM\",\n",
        "    scoring=\"average_precision\",\n",
        ")\n",
        "\n",
        "best_lgb = grid_lgb.best_estimator_\n"
      ],
      "metadata": {
        "id": "FFxIaPp4mdZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_proba = best_lgb.predict_proba(X_test_final)[:, 1]\n",
        "y_test_pred  = (y_test_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n LightGBM: Test Metrics\")\n",
        "print(f\"ROC-AUC : {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
        "print(f\"AUC-PR  : {average_precision_score(y_test, y_test_proba):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Recall  : {recall_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"F1      : {f1_score(y_test, y_test_pred):.4f}\")"
      ],
      "metadata": {
        "id": "XNQEZkhPV0h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## e. PyTorch MLP"
      ],
      "metadata": {
        "id": "2NPcEBiimNpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Convert to tensors\n",
        "X_train_t = torch.tensor(X_train_final, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_t  = torch.tensor(X_test_final, dtype=torch.float32).to(device)\n",
        "y_test_np = y_test  # keep numpy for metrics\n",
        "\n",
        "pos_weight = torch.tensor([scale_pos], dtype=torch.float32).to(device) # Convert the pos_weight that we have been using for other models\n"
      ],
      "metadata": {
        "id": "X1mLXaEvmljU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FraudMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_sizes=(256, 128), dropout=0.3):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_dim = input_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_dim, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            in_dim = h\n",
        "        layers.append(nn.Linear(in_dim, 1))  # binary output (logits)\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n"
      ],
      "metadata": {
        "id": "B2OuIa6pIAHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, yb in dataloader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = loss_fn(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def eval_average_precision(model, X_val, y_val):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_val.to(device))\n",
        "        proba  = torch.sigmoid(logits).cpu().numpy()\n",
        "    return average_precision_score(y_val, proba)\n"
      ],
      "metadata": {
        "id": "UuipnwaWY92K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_pytorch_mlp(\n",
        "    X_train_np,\n",
        "    y_train_np,\n",
        "    param_grid,\n",
        "    n_splits=5,\n",
        "    max_epochs=20,\n",
        "    batch_size=128,\n",
        "    patience=3,\n",
        "):\n",
        "    \"\"\"\n",
        "    X_train_np, y_train_np: numpy arrays\n",
        "    param_grid: dict like {\"hidden_sizes\":[...], \"dropout\":[...], \"lr\":[...]}\n",
        "    \"\"\"\n",
        "    input_dim = X_train_np.shape[1]\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    all_params = list(ParameterGrid(param_grid))\n",
        "    print(f\"\\nPyTorch MLP: {len(all_params)} candidates × {n_splits} folds = {len(all_params)*n_splits} fits\")\n",
        "    print(\"Scoring = average_precision (AUC-PR)\\n\")\n",
        "\n",
        "    best_score = -np.inf\n",
        "    best_params = None\n",
        "\n",
        "    for i, params in enumerate(all_params, start=1):\n",
        "        print(f\"\\n Candidate {i}/{len(all_params)}: {params}\")\n",
        "        fold_scores = []\n",
        "\n",
        "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_np, y_train_np), start=1):\n",
        "            # Split fold data\n",
        "            X_tr = torch.tensor(X_train_np[tr_idx], dtype=torch.float32)\n",
        "            y_tr = torch.tensor(y_train_np[tr_idx], dtype=torch.float32)\n",
        "            X_val = torch.tensor(X_train_np[val_idx], dtype=torch.float32)\n",
        "            y_val = y_train_np[val_idx]  # keep as numpy for metrics\n",
        "\n",
        "            # Dataloader\n",
        "            train_ds = TensorDataset(X_tr, y_tr)\n",
        "            train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            # Fresh model for each fold\n",
        "            model = FraudMLP(\n",
        "                input_dim=input_dim,\n",
        "                hidden_sizes=params[\"hidden_sizes\"],\n",
        "                dropout=params[\"dropout\"],\n",
        "            ).to(device)\n",
        "\n",
        "            optimizer = torch.optim.Adam(\n",
        "                model.parameters(),\n",
        "                lr=params[\"lr\"],\n",
        "            )\n",
        "            loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "            best_fold_loss = float(\"inf\")\n",
        "            patience_counter = 0\n",
        "\n",
        "            for epoch in range(max_epochs):\n",
        "                train_loss = train_one_epoch(model, train_dl, optimizer, loss_fn)\n",
        "\n",
        "                # Simple early stopping on training loss\n",
        "                if train_loss < best_fold_loss - 1e-4:\n",
        "                    best_fold_loss = train_loss\n",
        "                    patience_counter = 0\n",
        "                    best_state = model.state_dict()\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= patience:\n",
        "                        break\n",
        "\n",
        "            # Load best weights for evaluation\n",
        "            model.load_state_dict(best_state)\n",
        "\n",
        "            ap = eval_average_precision(model, X_val, y_val)\n",
        "            fold_scores.append(ap)\n",
        "            print(f\"  Fold {fold}: AUC-PR = {ap:.4f}\")\n",
        "\n",
        "        mean_ap = float(np.mean(fold_scores))\n",
        "        print(f\"--> Mean AUC-PR for {params}: {mean_ap:.4f}\")\n",
        "\n",
        "        if mean_ap > best_score:\n",
        "            best_score = mean_ap\n",
        "            best_params = params\n",
        "\n",
        "    print(\"\\n Best PyTorch MLP hyperparameters\")\n",
        "    print(\"Best params:\", best_params)\n",
        "    print(f\"Best mean CV AUC-PR: {best_score:.4f}\")\n",
        "\n",
        "    return best_params, best_score\n"
      ],
      "metadata": {
        "id": "dDw5PS2YZAD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_param_grid = {\n",
        "    \"hidden_sizes\": [\n",
        "        (256, 128),\n",
        "        (256, 128, 64),\n",
        "    ],\n",
        "    \"dropout\": [0.3, 0.5],\n",
        "    \"lr\": [1e-3, 5e-4],\n",
        "}\n",
        "\n",
        "best_params_mlp, best_cv_ap = grid_search_pytorch_mlp(\n",
        "    X_train_final,\n",
        "    y_train,\n",
        "    param_grid=mlp_param_grid,\n",
        "    n_splits=5,\n",
        "    max_epochs=20,\n",
        "    batch_size=128,\n",
        "    patience=3,\n",
        ")\n"
      ],
      "metadata": {
        "id": "PKDL60V7ZDAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final model using best hyperparameters\n",
        "input_dim = X_train_final.shape[1]\n",
        "\n",
        "final_mlp = FraudMLP(\n",
        "    input_dim=input_dim,\n",
        "    hidden_sizes=best_params_mlp[\"hidden_sizes\"],\n",
        "    dropout=best_params_mlp[\"dropout\"],\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    final_mlp.parameters(),\n",
        "    lr=best_params_mlp[\"lr\"],\n",
        ")\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Full-train dataset and loader\n",
        "full_train_ds = TensorDataset(X_train_t, y_train_t)\n",
        "full_train_dl = DataLoader(full_train_ds, batch_size=128, shuffle=True)\n",
        "\n",
        "epochs = 30\n",
        "best_loss = float(\"inf\")\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train_one_epoch(final_mlp, full_train_dl, optimizer, loss_fn)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} — loss = {train_loss:.4f}\")\n",
        "\n",
        "    if train_loss < best_loss - 1e-4:\n",
        "        best_loss = train_loss\n",
        "        patience_counter = 0\n",
        "        best_state = final_mlp.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping on full train.\")\n",
        "            break\n",
        "\n",
        "final_mlp.load_state_dict(best_state)"
      ],
      "metadata": {
        "id": "YIv7PDZ4IDzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test evaluation\n",
        "final_mlp.eval()\n",
        "with torch.no_grad():\n",
        "    logits_test = final_mlp(X_test_t)\n",
        "    proba_test = torch.sigmoid(logits_test).cpu().numpy()\n",
        "\n",
        "y_test_pred = (proba_test >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\nPyTorch MLP: Test Metrics\")\n",
        "print(f\"ROC-AUC : {roc_auc_score(y_test_np, proba_test):.4f}\")\n",
        "print(f\"AUC-PR  : {average_precision_score(y_test_np, proba_test):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_np, y_test_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test_np, y_test_pred):.4f}\")\n",
        "print(f\"Recall  : {recall_score(y_test_np, y_test_pred):.4f}\")\n",
        "print(f\"F1      : {f1_score(y_test_np, y_test_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "AnbZYbJB-OEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Ensemble"
      ],
      "metadata": {
        "id": "FaAQfz40aenq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_mlp_params = {'dropout': 0.5, 'hidden_sizes': (256, 128, 64), 'lr': 0.0005} # Taken from MLP grid search"
      ],
      "metadata": {
        "id": "RUp96m4N2s4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cv_pred_probas_sklearn(estimator, X, y, n_splits=5):\n",
        "    \"\"\"\n",
        "    Build a probability vector for all training samples using\n",
        "    5-fold CV: each sample's prediction comes from a fold where it\n",
        "    was in the validation part (not used to train that fold's model).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "    preds = np.zeros(len(y), dtype=float)\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y), start=1):\n",
        "        print(f\"[{estimator.__class__.__name__}] Fold {fold}\")\n",
        "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
        "        y_tr = y[tr_idx]\n",
        "\n",
        "        model = clone(estimator) # same hyperparams, fresh fit\n",
        "        model.fit(X_tr, y_tr)\n",
        "        preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "    return preds\n"
      ],
      "metadata": {
        "id": "DnP1bT9syxVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cv_pred_probas_mlp(\n",
        "    X,\n",
        "    y,\n",
        "    n_splits=5,\n",
        "    hidden_sizes=(256, 128, 64),\n",
        "    dropout=0.3,\n",
        "    lr=1e-3,\n",
        "    batch_size=128,\n",
        "    max_epochs=20,\n",
        "    patience=3,\n",
        "):\n",
        "    X_np = np.asarray(X)\n",
        "    y_np = np.asarray(y)\n",
        "    input_dim = X_np.shape[1]\n",
        "\n",
        "    preds = np.zeros(len(y_np), dtype=float)\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_np, y_np), start=1):\n",
        "        print(f\"[MLP] Fold {fold}\")\n",
        "\n",
        "        X_tr = torch.tensor(X_np[tr_idx], dtype=torch.float32)\n",
        "        y_tr = torch.tensor(y_np[tr_idx], dtype=torch.float32)\n",
        "        X_val = torch.tensor(X_np[val_idx], dtype=torch.float32)\n",
        "\n",
        "        train_ds = TensorDataset(X_tr, y_tr)\n",
        "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        model = FraudMLP(\n",
        "            input_dim=input_dim,\n",
        "            hidden_sizes=hidden_sizes,\n",
        "            dropout=dropout,\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "        best_loss = float(\"inf\")\n",
        "        best_state = None\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            train_loss = train_one_epoch(model, train_dl, optimizer, loss_fn)\n",
        "\n",
        "            if train_loss < best_loss - 1e-4:\n",
        "                best_loss = train_loss\n",
        "                best_state = model.state_dict()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    break\n",
        "\n",
        "        if best_state is not None:\n",
        "            model.load_state_dict(best_state)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits_val = model(X_val.to(device))\n",
        "            proba_val = torch.sigmoid(logits_val).cpu().numpy().ravel()\n",
        "\n",
        "        preds[val_idx] = proba_val\n",
        "\n",
        "    return preds\n"
      ],
      "metadata": {
        "id": "T5KWpkvKyzDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation predictions on train for each base model\n",
        "best_lgb.set_params(verbose=-1) # Silence LightGBM warnings\n",
        "cv_pred_lgb = get_cv_pred_probas_sklearn(best_lgb, X_train_final, y_train, n_splits=5)\n",
        "cv_pred_rf  = get_cv_pred_probas_sklearn(best_rf,  X_train_final, y_train, n_splits=5)\n",
        "\n",
        "cv_pred_mlp = get_cv_pred_probas_mlp(\n",
        "    X_train_final,\n",
        "    y_train,\n",
        "    n_splits=5,\n",
        "    hidden_sizes=best_mlp_params[\"hidden_sizes\"],\n",
        "    dropout=best_mlp_params[\"dropout\"],\n",
        "    lr=best_mlp_params[\"lr\"],\n",
        ")\n",
        "\n",
        "# Stack them as features for the meta-model\n",
        "Z_train = np.column_stack([cv_pred_lgb, cv_pred_rf, cv_pred_mlp])\n",
        "print(\"Z_train shape:\", Z_train.shape)\n"
      ],
      "metadata": {
        "id": "gcZxtE3By1B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the meta-model (Logistic Regression) on stacked train features\n",
        "meta_lr = LogisticRegression(\n",
        "    penalty=\"l2\",\n",
        "    solver=\"lbfgs\",\n",
        "    max_iter=5000,\n",
        "    class_weight={0: 1.0, 1: scale_pos},  # reuse the imbalance ratio\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "meta_lr.fit(Z_train, y_train)\n",
        "\n",
        "# Retrain base models on FULL train set with best hyperparameters\n",
        "# LightGBM\n",
        "lgb_full = clone(best_lgb).fit(X_train_final, y_train)\n",
        "\n",
        "# Random Forest\n",
        "rf_full = clone(best_rf).fit(X_train_final, y_train)\n",
        "\n",
        "# MLP\n",
        "X_train_t = torch.tensor(X_train_final, dtype=torch.float32).to(device)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "\n",
        "train_ds_full = TensorDataset(X_train_t, y_train_t)\n",
        "train_dl_full = DataLoader(train_ds_full, batch_size=128, shuffle=True)\n",
        "\n",
        "mlp_full = FraudMLP(\n",
        "    input_dim=X_train_final.shape[1],\n",
        "    hidden_sizes=best_mlp_params[\"hidden_sizes\"],\n",
        "    dropout=best_mlp_params[\"dropout\"],\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(mlp_full.parameters(), lr=best_mlp_params[\"lr\"])\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "best_loss = float(\"inf\")\n",
        "best_state = None\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(30):\n",
        "    train_loss = train_one_epoch(mlp_full, train_dl_full, optimizer, loss_fn)\n",
        "    print(f\"[MLP full] Epoch {epoch+1}/30 — loss = {train_loss:.4f}\")\n",
        "\n",
        "    if train_loss < best_loss - 1e-4:\n",
        "        best_loss = train_loss\n",
        "        best_state = mlp_full.state_dict()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping on full train.\")\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    mlp_full.load_state_dict(best_state)\n",
        "\n",
        "# Get base-model probabilities on test set and stack into Z_test\n",
        "proba_test_lgb = lgb_full.predict_proba(X_test_final)[:, 1]\n",
        "proba_test_rf  = rf_full.predict_proba(X_test_final)[:, 1]\n",
        "\n",
        "X_test_t = torch.tensor(X_test_final, dtype=torch.float32).to(device)\n",
        "mlp_full.eval()\n",
        "with torch.no_grad():\n",
        "    logits_test_mlp = mlp_full(X_test_t)\n",
        "    proba_test_mlp  = torch.sigmoid(logits_test_mlp).cpu().numpy().ravel()\n",
        "\n",
        "Z_test = np.column_stack([proba_test_lgb, proba_test_rf, proba_test_mlp])\n",
        "print(\"Z_test shape:\", Z_test.shape)\n",
        "\n",
        "# Meta-model final prediction + metrics\n",
        "meta_proba = meta_lr.predict_proba(Z_test)[:, 1]\n",
        "meta_pred  = (meta_proba >= 0.5).astype(int)\n",
        "\n",
        "print(\"\\n Stacked Ensemble (LGB + RF + MLP) — Test Metrics\")\n",
        "print(f\"ROC-AUC : {roc_auc_score(y_test, meta_proba):.4f}\")\n",
        "print(f\"AUC-PR  : {average_precision_score(y_test, meta_proba):.4f}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, meta_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, meta_pred):.4f}\")\n",
        "print(f\"Recall  : {recall_score(y_test, meta_pred):.4f}\")\n",
        "print(f\"F1      : {f1_score(y_test, meta_pred):.4f}\")\n"
      ],
      "metadata": {
        "id": "GkJESMDn4uoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. AUC Curves"
      ],
      "metadata": {
        "id": "HOPomU3ng0Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather all model probabilities in a dict\n",
        "proba_lr = best_lr.predict_proba(X_test_final)[:, 1]\n",
        "proba_svm = best_svm.predict_proba(X_test_final)[:, 1]\n",
        "proba_rf = best_rf.predict_proba(X_test_final)[:, 1]\n",
        "proba_lgb = best_lgb.predict_proba(X_test_final)[:, 1]\n",
        "proba_mlp = proba_test\n",
        "proba_stack = meta_proba\n",
        "\n",
        "model_probas = {\n",
        "    \"Logistic Regression\": proba_lr,\n",
        "    \"SVM\":                 proba_svm,\n",
        "    \"Random Forest\":       proba_rf,\n",
        "    \"LightGBM\":            proba_lgb,\n",
        "    \"MLP\":                 proba_mlp,\n",
        "    \"Stacked Ensemble\":    proba_stack,\n",
        "}\n",
        "\n",
        "# Plot the ROC curves for all models in one figure\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for name, proba in model_probas.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.3f})\")\n",
        "\n",
        "# Chance line\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves for All Models\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot the Precision–Recall curves for all models in one figure\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for name, proba in model_probas.items():\n",
        "    precision, recall, _ = precision_recall_curve(y_test, proba)\n",
        "    ap = average_precision_score(y_test, proba)\n",
        "    plt.plot(recall, precision, label=f\"{name} (AP = {ap:.3f})\")\n",
        "\n",
        "# Baseline: positive rate\n",
        "pos_rate = (y_test == 1).mean()\n",
        "plt.hlines(\n",
        "    pos_rate,\n",
        "    0, 1,\n",
        "    linestyles=\"--\",\n",
        "    linewidth=1,\n",
        "    label=f\"Baseline (pos rate = {pos_rate:.3f})\"\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision–Recall Curves for All Models\")\n",
        "plt.legend(loc=\"lower left\", fontsize=8)\n",
        "\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8g7XCgnEg5dh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
